{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d104e0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using analogy file: word-test.v1.txt\n",
      "Found sections in file (sample): ['capital-common-countries', 'capital-world', 'city-in-state', 'currency', 'family', 'gram1-adjective-to-adverb', 'gram2-opposite', 'gram3-comparative', 'gram4-superlative', 'gram5-present-participle'] ...\n",
      "Semantic questions: 506\n",
      "Syntactic questions: 1560\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Try these paths in order (edit if needed)\n",
    "CANDIDATE_PATHS = [\n",
    "    \"word-test.v1.txt\",                 # same directory as notebook\n",
    "    \"./word-test.v1.txt\",\n",
    "    \"/mnt/data/word-test.v1.txt\",       # common mounted path in this environment\n",
    "]\n",
    "\n",
    "SEMANTIC_SECTION = \"capital-common-countries\"\n",
    "SYNTACTIC_SECTION = \"gram7-past-tense\"\n",
    "\n",
    "def resolve_path(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find word-test.v1.txt. Tried:\\n\" + \"\\n\".join(paths) +\n",
    "        \"\\n\\nTip: put the file in the same folder as the notebook OR set ANALOGY_PATH to the correct location.\"\n",
    "    )\n",
    "\n",
    "ANALOGY_PATH = resolve_path(CANDIDATE_PATHS)\n",
    "print(\"Using analogy file:\", ANALOGY_PATH)\n",
    "\n",
    "def load_analogy_questions(path: str, keep_sections) -> Dict[str, List[Tuple[str,str,str,str]]]:\n",
    "    keep_sections = set(keep_sections)\n",
    "    out = {s: [] for s in keep_sections}\n",
    "    current = None\n",
    "    found_sections = set()\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            if line.startswith(\":\"):\n",
    "                current = line[1:].strip()\n",
    "                found_sections.add(current)\n",
    "                continue\n",
    "\n",
    "            if current not in keep_sections:\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "            if len(parts) != 4:\n",
    "                continue\n",
    "            out[current].append(tuple(parts))\n",
    "\n",
    "    print(\"Found sections in file (sample):\", sorted(list(found_sections))[:10], \"...\")\n",
    "    return out\n",
    "\n",
    "sections = load_analogy_questions(ANALOGY_PATH, [SEMANTIC_SECTION, SYNTACTIC_SECTION])\n",
    "\n",
    "print(\"Semantic questions:\", len(sections[SEMANTIC_SECTION]))\n",
    "print(\"Syntactic questions:\", len(sections[SYNTACTIC_SECTION]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab13b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Setup\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea1b4fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 1560)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2 — Load required sections from the word analogy file\n",
    "\n",
    "ANALOGY_PATH = \"word-test.v1.txt\"\n",
    "SEMANTIC_SECTION = \"capital-common-countries\"\n",
    "SYNTACTIC_SECTION = \"gram7-past-tense\"\n",
    "\n",
    "def load_analogy_questions(path: str, keep_sections) -> Dict[str, List[Tuple[str,str,str,str]]]:\n",
    "    keep_sections = set(keep_sections)\n",
    "    out = {s: [] for s in keep_sections}\n",
    "    current = None\n",
    "    \n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\":\"):\n",
    "                current = line[1:].strip()\n",
    "                continue\n",
    "            if current not in keep_sections:\n",
    "                continue\n",
    "            \n",
    "            parts = line.split()\n",
    "            if len(parts) != 4:\n",
    "                continue\n",
    "            out[current].append(tuple(parts))\n",
    "    return out\n",
    "\n",
    "sections = load_analogy_questions(ANALOGY_PATH, [SEMANTIC_SECTION, SYNTACTIC_SECTION])\n",
    "len(sections[SEMANTIC_SECTION]), len(sections[SYNTACTIC_SECTION])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c894c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.7682328820228577),\n",
       " ('queen', 0.7507690787315369),\n",
       " ('son', 0.7020888328552246),\n",
       " ('brother', 0.6985775232315063),\n",
       " ('monarch', 0.6977890729904175)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3 — Load pretrained GloVe (Gensim)\n",
    "# NOTE: This requires internet the first time (it downloads the vectors).\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Common choices:\n",
    "#   glove-wiki-gigaword-50\n",
    "#   glove-wiki-gigaword-100\n",
    "#   glove-wiki-gigaword-200\n",
    "#   glove-wiki-gigaword-300\n",
    "kv = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# quick sanity check\n",
    "kv.most_similar(\"king\", topn=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b59c342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic (capital-common-countries)  accuracy: 0.0000  used=0  skipped=506\n",
      "Syntactic (gram7-past-tense) accuracy: 0.5545  used=1560  skipped=0\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — Fast analogy evaluation on KeyedVectors (cosine via normalized dot)\n",
    "\n",
    "def evaluate_analogies_kv(kv, questions: List[Tuple[str,str,str,str]]):\n",
    "    total_used = 0\n",
    "    correct = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    # Pre-normalize for fast cosine search\n",
    "    words = kv.index_to_key\n",
    "    mat = kv.vectors.astype(np.float32)\n",
    "    norms = np.linalg.norm(mat, axis=1, keepdims=True)\n",
    "    mat_norm = mat / np.maximum(norms, 1e-12)\n",
    "\n",
    "    key_to_index = kv.key_to_index\n",
    "    \n",
    "    def has(w): \n",
    "        return w in key_to_index\n",
    "\n",
    "    for a, b, c, d in questions:\n",
    "        if not (has(a) and has(b) and has(c) and has(d)):\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        va = mat_norm[key_to_index[a]]\n",
    "        vb = mat_norm[key_to_index[b]]\n",
    "        vc = mat_norm[key_to_index[c]]\n",
    "\n",
    "        q = vb - va + vc\n",
    "        q = q / max(np.linalg.norm(q), 1e-12)\n",
    "\n",
    "        sims = mat_norm @ q\n",
    "        \n",
    "        # exclude a,b,c\n",
    "        sims[key_to_index[a]] = -np.inf\n",
    "        sims[key_to_index[b]] = -np.inf\n",
    "        sims[key_to_index[c]] = -np.inf\n",
    "        \n",
    "        pred_idx = int(np.argmax(sims))\n",
    "        pred = words[pred_idx]\n",
    "        \n",
    "        total_used += 1\n",
    "        if pred == d:\n",
    "            correct += 1\n",
    "\n",
    "    acc = correct / total_used if total_used > 0 else 0.0\n",
    "    return {\n",
    "        \"used\": total_used,\n",
    "        \"correct\": correct,\n",
    "        \"accuracy\": acc,\n",
    "        \"skipped_oov\": skipped,\n",
    "        \"total_in_section\": len(questions),\n",
    "    }\n",
    "\n",
    "sem_res = evaluate_analogies_kv(kv, sections[SEMANTIC_SECTION])\n",
    "syn_res = evaluate_analogies_kv(kv, sections[SYNTACTIC_SECTION])\n",
    "\n",
    "print(f\"Semantic ({SEMANTIC_SECTION})  accuracy: {sem_res['accuracy']:.4f}  used={sem_res['used']}  skipped={sem_res['skipped_oov']}\")\n",
    "print(f\"Syntactic ({SYNTACTIC_SECTION}) accuracy: {syn_res['accuracy']:.4f}  used={syn_res['used']}  skipped={syn_res['skipped_oov']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0a883dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math, time, random\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d145f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic questions: 506\n",
      "Syntactic questions: 1560\n"
     ]
    }
   ],
   "source": [
    "ANALOGY_PATH = \"word-test.v1.txt\"\n",
    "SEMANTIC_SECTION = \"capital-common-countries\"\n",
    "SYNTACTIC_SECTION = \"gram7-past-tense\"\n",
    "\n",
    "def load_analogy_questions(path: str, keep_sections) -> Dict[str, List[Tuple[str,str,str,str]]]:\n",
    "    keep_sections = set(keep_sections)\n",
    "    out = {s: [] for s in keep_sections}\n",
    "    current = None\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\":\"):\n",
    "                current = line[1:].strip()\n",
    "                continue\n",
    "            if current not in keep_sections:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) == 4:\n",
    "                out[current].append(tuple(parts))\n",
    "    return out\n",
    "\n",
    "sections = load_analogy_questions(ANALOGY_PATH, [SEMANTIC_SECTION, SYNTACTIC_SECTION])\n",
    "print(\"Semantic questions:\", len(sections[SEMANTIC_SECTION]))\n",
    "print(\"Syntactic questions:\", len(sections[SYNTACTIC_SECTION]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff622f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num sentences: 3044\n"
     ]
    }
   ],
   "source": [
    "# If you have raw text file:\n",
    "RAW_TEXT_PATH = \"archive/SorcerersStone.txt\"\n",
    "\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s']\", \" \", text)\n",
    "    return text.split()\n",
    "\n",
    "def load_sentences_from_textfile(path: str, max_lines=None) -> List[List[str]]:\n",
    "    sents = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_lines is not None and i >= max_lines:\n",
    "                break\n",
    "            toks = simple_tokenize(line)\n",
    "            if len(toks) >= 2:\n",
    "                sents.append(toks)\n",
    "    return sents\n",
    "\n",
    "sentences = load_sentences_from_textfile(RAW_TEXT_PATH)\n",
    "print(\"num sentences:\", len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fae3558c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 3355 Encoded sentences: 3033\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(sentences: List[List[str]], min_count: int = 2):\n",
    "    freq = {}\n",
    "    for sent in sentences:\n",
    "        for w in sent:\n",
    "            freq[w] = freq.get(w, 0) + 1\n",
    "    vocab = [w for w,c in freq.items() if c >= min_count]\n",
    "    vocab.sort()\n",
    "    word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "    idx2word = vocab\n",
    "    return word2idx, idx2word, freq\n",
    "\n",
    "def encode_sentences(sentences: List[List[str]], word2idx: Dict[str,int]) -> List[List[int]]:\n",
    "    enc = []\n",
    "    for sent in sentences:\n",
    "        ids = [word2idx[w] for w in sent if w in word2idx]\n",
    "        if len(ids) >= 2:\n",
    "            enc.append(ids)\n",
    "    return enc\n",
    "\n",
    "# === YOU MUST have 'sentences' defined by now ===\n",
    "word2idx, idx2word, freq = build_vocab(sentences, min_count=2)\n",
    "encoded = encode_sentences(sentences, word2idx)\n",
    "\n",
    "V = len(idx2word)\n",
    "print(\"Vocab size:\", V, \"Encoded sentences:\", len(encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc97ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x = np.clip(x, -20, 20)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def stable_softmax(x):\n",
    "    x = x - np.max(x)\n",
    "    ex = np.exp(np.clip(x, -20, 20))\n",
    "    return ex / np.sum(ex)\n",
    "\n",
    "def generate_skipgram_pairs(encoded_sents: List[List[int]], window_size: int):\n",
    "    pairs = []\n",
    "    for sent in encoded_sents:\n",
    "        n = len(sent)\n",
    "        for i, center in enumerate(sent):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(n, i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if j == i:\n",
    "                    continue\n",
    "                context = sent[j]\n",
    "                pairs.append((center, context))\n",
    "    return pairs\n",
    "\n",
    "def evaluate_analogies_numpy(vectors: np.ndarray, word2idx: Dict[str,int], idx2word: List[str],\n",
    "                            questions: List[Tuple[str,str,str,str]]):\n",
    "    # vectors: (V,D)\n",
    "    eps = 1e-12\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    mat = vectors / np.maximum(norms, eps)\n",
    "\n",
    "    used = correct = skipped = 0\n",
    "    for a,b,c,d in questions:\n",
    "        if not (a in word2idx and b in word2idx and c in word2idx and d in word2idx):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        ia, ib, ic, id_ = word2idx[a], word2idx[b], word2idx[c], word2idx[d]\n",
    "        q = mat[ib] - mat[ia] + mat[ic]\n",
    "        q = q / max(np.linalg.norm(q), eps)\n",
    "\n",
    "        sims = mat @ q\n",
    "        sims[ia] = sims[ib] = sims[ic] = -np.inf\n",
    "        pred = int(np.argmax(sims))\n",
    "\n",
    "        used += 1\n",
    "        if pred == id_:\n",
    "            correct += 1\n",
    "\n",
    "    acc = correct / used if used > 0 else 0.0\n",
    "    return {\"used\": used, \"correct\": correct, \"accuracy\": acc, \"skipped\": skipped}\n",
    "\n",
    "def eval_sem_syn(vectors, word2idx, idx2word):\n",
    "    sem = evaluate_analogies_numpy(vectors, word2idx, idx2word, sections[SEMANTIC_SECTION])\n",
    "    syn = evaluate_analogies_numpy(vectors, word2idx, idx2word, sections[SYNTACTIC_SECTION])\n",
    "    return sem, syn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a187095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram_softmax(encoded_sents, V, D=100, window_size=5, epochs=1, lr=0.05, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    W_in  = (rng.standard_normal((V, D)) * 0.01).astype(np.float32)\n",
    "    W_out = (rng.standard_normal((V, D)) * 0.01).astype(np.float32)\n",
    "\n",
    "    pairs = generate_skipgram_pairs(encoded_sents, window_size)\n",
    "    rng.shuffle(pairs)\n",
    "\n",
    "    start_time = time.time()\n",
    "    last_loss = None\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        rng.shuffle(pairs)\n",
    "\n",
    "        for center, target in pairs:\n",
    "            v = W_in[center]          # (D,)\n",
    "            scores = W_out @ v        # (V,)\n",
    "            probs = stable_softmax(scores)\n",
    "\n",
    "            loss = -math.log(max(probs[target], 1e-12))\n",
    "            total_loss += loss\n",
    "\n",
    "            # gradients\n",
    "            ds = probs\n",
    "            ds[target] -= 1.0         # (V,)\n",
    "            grad_v = W_out.T @ ds     # (D,)\n",
    "\n",
    "            W_out -= lr * (ds[:, None] * v[None, :])\n",
    "            W_in[center] -= lr * grad_v\n",
    "\n",
    "        last_loss = total_loss / max(len(pairs), 1)\n",
    "        print(f\"[Skipgram Softmax] epoch {ep+1}/{epochs} avg_loss={last_loss:.4f}\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    return W_in, last_loss, elapsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3670aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unigram_table(freq: Dict[str,int], word2idx: Dict[str,int], power=0.75):\n",
    "    # returns distribution over indices\n",
    "    probs = np.zeros(len(word2idx), dtype=np.float64)\n",
    "    for w,i in word2idx.items():\n",
    "        probs[i] = (freq[w] ** power)\n",
    "    probs /= probs.sum()\n",
    "    return probs\n",
    "\n",
    "def train_skipgram_neg(encoded_sents, V, freq, word2idx, D=100, window_size=5, epochs=2, lr=0.05, K=5, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    W_in  = (rng.standard_normal((V, D)) * 0.01).astype(np.float32)\n",
    "    W_out = (rng.standard_normal((V, D)) * 0.01).astype(np.float32)\n",
    "\n",
    "    neg_dist = make_unigram_table(freq, word2idx)\n",
    "    pairs = generate_skipgram_pairs(encoded_sents, window_size)\n",
    "\n",
    "    start_time = time.time()\n",
    "    last_loss = None\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        rng.shuffle(pairs)\n",
    "\n",
    "        for center, pos in pairs:\n",
    "            v = W_in[center]     # (D,)\n",
    "\n",
    "            # positive\n",
    "            u_pos = W_out[pos]\n",
    "            score_pos = float(u_pos @ v)\n",
    "            loss_pos = -math.log(max(sigmoid(score_pos), 1e-12))\n",
    "\n",
    "            # negatives (sample indices)\n",
    "            negs = rng.choice(V, size=K, replace=True, p=neg_dist)\n",
    "            u_negs = W_out[negs]                 # (K,D)\n",
    "            score_negs = u_negs @ v              # (K,)\n",
    "            loss_negs = -np.sum(np.log(np.maximum(sigmoid(-score_negs), 1e-12)))\n",
    "\n",
    "            loss = loss_pos + float(loss_negs)\n",
    "            total_loss += loss\n",
    "\n",
    "            # gradients\n",
    "            g_pos = sigmoid(score_pos) - 1.0     # d/dscore for pos\n",
    "            grad_v = g_pos * u_pos\n",
    "\n",
    "            W_out[pos] -= lr * (g_pos * v)\n",
    "\n",
    "            g_negs = sigmoid(score_negs)         # because loss has -log(sigmoid(-s)) => sigmoid(s)\n",
    "            grad_v += (g_negs[:, None] * u_negs).sum(axis=0)\n",
    "\n",
    "            W_out[negs] -= lr * (g_negs[:, None] * v[None, :])\n",
    "\n",
    "            W_in[center] -= lr * grad_v\n",
    "\n",
    "        last_loss = total_loss / max(len(pairs), 1)\n",
    "        print(f\"[Skipgram NEG] epoch {ep+1}/{epochs} avg_loss={last_loss:.4f}\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    return W_in, last_loss, elapsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a62453ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence(encoded_sents, V, window_size=5):\n",
    "    # Sparse dict: (i,j) -> X_ij\n",
    "    X = {}\n",
    "    for sent in encoded_sents:\n",
    "        n = len(sent)\n",
    "        for i, wi in enumerate(sent):\n",
    "            start = max(0, i - window_size)\n",
    "            end   = min(n, i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if j == i:\n",
    "                    continue\n",
    "                wj = sent[j]\n",
    "                dist = abs(j - i)\n",
    "                inc = 1.0 / dist  # common GloVe weighting\n",
    "                X[(wi, wj)] = X.get((wi, wj), 0.0) + inc\n",
    "    return X\n",
    "\n",
    "def train_glove(encoded_sents, V, D=100, window_size=5, epochs=20, lr=0.05, x_max=100.0, alpha=0.75, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    W = (rng.standard_normal((V, D)) * 0.01).astype(np.float32)\n",
    "    C = (rng.standard_normal((V, D)) * 0.01).astype(np.float32)\n",
    "    bW = np.zeros(V, dtype=np.float32)\n",
    "    bC = np.zeros(V, dtype=np.float32)\n",
    "\n",
    "    # AdaGrad accumulators\n",
    "    gW  = np.ones((V, D), dtype=np.float32)\n",
    "    gC  = np.ones((V, D), dtype=np.float32)\n",
    "    gbW = np.ones(V, dtype=np.float32)\n",
    "    gbC = np.ones(V, dtype=np.float32)\n",
    "\n",
    "    X = build_cooccurrence(encoded_sents, V, window_size)\n",
    "    items = list(X.items())\n",
    "\n",
    "    start_time = time.time()\n",
    "    last_loss = None\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        random.shuffle(items)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for (i, j), xij in items:\n",
    "            w = (xij / x_max) ** alpha if xij < x_max else 1.0\n",
    "            logx = math.log(max(xij, 1e-12))\n",
    "\n",
    "            pred = float(W[i] @ C[j] + bW[i] + bC[j])\n",
    "            diff = pred - logx\n",
    "            loss = w * (diff ** 2)\n",
    "            total_loss += loss\n",
    "\n",
    "            # gradients\n",
    "            grad = 2.0 * w * diff\n",
    "            dWi = grad * C[j]\n",
    "            dCj = grad * W[i]\n",
    "            dbWi = grad\n",
    "            dbCj = grad\n",
    "\n",
    "            # AdaGrad update\n",
    "            gW[i]  += dWi * dWi\n",
    "            gC[j]  += dCj * dCj\n",
    "            gbW[i] += dbWi * dbWi\n",
    "            gbC[j] += dbCj * dbCj\n",
    "\n",
    "            W[i]  -= (lr / np.sqrt(gW[i]))  * dWi\n",
    "            C[j]  -= (lr / np.sqrt(gC[j]))  * dCj\n",
    "            bW[i] -= (lr / math.sqrt(float(gbW[i]))) * dbWi\n",
    "            bC[j] -= (lr / math.sqrt(float(gbC[j]))) * dbCj\n",
    "\n",
    "        last_loss = total_loss / max(len(items), 1)\n",
    "        if (ep+1) % max(1, epochs//5) == 0 or ep == 0:\n",
    "            print(f\"[GloVe] epoch {ep+1}/{epochs} avg_loss={last_loss:.4f}\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    # Standard practice: use W + C as final embedding\n",
    "    vectors = (W + C).astype(np.float32)\n",
    "    return vectors, last_loss, elapsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68eb4084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipgram Softmax] epoch 1/1 avg_loss=6.3748\n",
      "[Skipgram NEG] epoch 1/2 avg_loss=2.7267\n",
      "[Skipgram NEG] epoch 2/2 avg_loss=2.5282\n",
      "[GloVe] epoch 1/20 avg_loss=0.0549\n",
      "[GloVe] epoch 4/20 avg_loss=0.0246\n",
      "[GloVe] epoch 8/20 avg_loss=0.0197\n",
      "[GloVe] epoch 12/20 avg_loss=0.0166\n",
      "[GloVe] epoch 16/20 avg_loss=0.0144\n",
      "[GloVe] epoch 20/20 avg_loss=0.0129\n",
      "\n",
      "Model\t\t\tWindow\tTraining Loss\tTraining Time(s)\tSyntactic Acc\tSemantic Acc\n",
      "Skipgram        \t5\t6.3748\t\t362.88\t\t\t0.0000\t\t0.0000\n",
      "Skipgram (NEG)  \t5\t2.5282\t\t98.00\t\t\t0.0040\t\t0.0000\n",
      "GloVe (scratch) \t5\t0.0129\t\t58.02\t\t\t0.0059\t\t0.0000\n"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE = 5\n",
    "D = 100\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1) Skipgram Softmax\n",
    "W_sg, loss_sg, t_sg = train_skipgram_softmax(encoded, V, D=D, window_size=WINDOW_SIZE, epochs=1, lr=0.05)\n",
    "sem_sg, syn_sg = eval_sem_syn(W_sg, word2idx, idx2word)\n",
    "results.append((\"Skipgram\", WINDOW_SIZE, loss_sg, t_sg, syn_sg[\"accuracy\"], sem_sg[\"accuracy\"]))\n",
    "\n",
    "# 2) Skipgram NEG\n",
    "W_sgns, loss_sgns, t_sgns = train_skipgram_neg(encoded, V, freq, word2idx, D=D, window_size=WINDOW_SIZE, epochs=2, lr=0.05, K=5)\n",
    "sem_sgns, syn_sgns = eval_sem_syn(W_sgns, word2idx, idx2word)\n",
    "results.append((\"Skipgram (NEG)\", WINDOW_SIZE, loss_sgns, t_sgns, syn_sgns[\"accuracy\"], sem_sgns[\"accuracy\"]))\n",
    "\n",
    "# 3) GloVe scratch\n",
    "W_glove, loss_glove, t_glove = train_glove(encoded, V, D=D, window_size=WINDOW_SIZE, epochs=20, lr=0.05)\n",
    "sem_glove, syn_glove = eval_sem_syn(W_glove, word2idx, idx2word)\n",
    "results.append((\"GloVe (scratch)\", WINDOW_SIZE, loss_glove, t_glove, syn_glove[\"accuracy\"], sem_glove[\"accuracy\"]))\n",
    "\n",
    "# Print table\n",
    "print(\"\\nModel\\t\\t\\tWindow\\tTraining Loss\\tTraining Time(s)\\tSyntactic Acc\\tSemantic Acc\")\n",
    "for r in results:\n",
    "    print(f\"{r[0]:<16}\\t{r[1]}\\t{r[2]:.4f}\\t\\t{r[3]:.2f}\\t\\t\\t{r[4]:.4f}\\t\\t{r[5]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c09350e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ADD-ON 1: GloVe (Gensim) row (4th row)\n",
    "# ADD-ON 2: WordSim353 CSV -> MSE (+ Spearman, optional)\n",
    "# =========================\n",
    "\n",
    "import os, math, time, random, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ---------- Analogy helpers (works for numpy vectors + your vocab) ----------\n",
    "def evaluate_analogies_numpy(vectors: np.ndarray, word2idx: Dict[str,int], idx2word: List[str],\n",
    "                            questions: List[Tuple[str,str,str,str]]):\n",
    "    eps = 1e-12\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    mat = vectors / np.maximum(norms, eps)\n",
    "\n",
    "    used = correct = skipped = 0\n",
    "    for a,b,c,d in questions:\n",
    "        if not (a in word2idx and b in word2idx and c in word2idx and d in word2idx):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        ia, ib, ic, id_ = word2idx[a], word2idx[b], word2idx[c], word2idx[d]\n",
    "        q = mat[ib] - mat[ia] + mat[ic]\n",
    "        q = q / max(np.linalg.norm(q), eps)\n",
    "\n",
    "        sims = mat @ q\n",
    "        sims[ia] = sims[ib] = sims[ic] = -np.inf\n",
    "        pred = int(np.argmax(sims))\n",
    "\n",
    "        used += 1\n",
    "        if pred == id_:\n",
    "            correct += 1\n",
    "\n",
    "    acc = correct / used if used > 0 else 0.0\n",
    "    return {\"used\": used, \"correct\": correct, \"accuracy\": acc, \"skipped\": skipped}\n",
    "\n",
    "def eval_sem_syn_numpy(vectors, word2idx, idx2word, sections, sem_section, syn_section):\n",
    "    sem = evaluate_analogies_numpy(vectors, word2idx, idx2word, sections[sem_section])\n",
    "    syn = evaluate_analogies_numpy(vectors, word2idx, idx2word, sections[syn_section])\n",
    "    return sem, syn\n",
    "\n",
    "# ---------- Analogy helpers for gensim KeyedVectors ----------\n",
    "def evaluate_analogies_kv(kv, questions: List[Tuple[str,str,str,str]]):\n",
    "    total_used = 0\n",
    "    correct = 0\n",
    "    skipped = 0\n",
    "\n",
    "    words = kv.index_to_key\n",
    "    mat = kv.vectors.astype(np.float32)\n",
    "    norms = np.linalg.norm(mat, axis=1, keepdims=True)\n",
    "    mat_norm = mat / np.maximum(norms, 1e-12)\n",
    "    key_to_index = kv.key_to_index\n",
    "\n",
    "    def has(w): \n",
    "        return w in key_to_index\n",
    "\n",
    "    for a, b, c, d in questions:\n",
    "        if not (has(a) and has(b) and has(c) and has(d)):\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        ia, ib, ic, id_ = key_to_index[a], key_to_index[b], key_to_index[c], key_to_index[d]\n",
    "        q = mat_norm[ib] - mat_norm[ia] + mat_norm[ic]\n",
    "        q = q / max(np.linalg.norm(q), 1e-12)\n",
    "\n",
    "        sims = mat_norm @ q\n",
    "        sims[ia] = sims[ib] = sims[ic] = -np.inf\n",
    "        pred_idx = int(np.argmax(sims))\n",
    "        pred = words[pred_idx]\n",
    "\n",
    "        total_used += 1\n",
    "        if pred == d:\n",
    "            correct += 1\n",
    "\n",
    "    acc = correct / total_used if total_used > 0 else 0.0\n",
    "    return {\"used\": total_used, \"correct\": correct, \"accuracy\": acc, \"skipped\": skipped}\n",
    "\n",
    "# ---------- WordSim353: load + compute MSE (and Spearman) ----------\n",
    "def load_wordsim353_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust loader for WordSim353 CSV variants.\n",
    "    Tries to detect columns for word1, word2, score.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # normalize column names\n",
    "    cols = {c: re.sub(r\"\\s+\", \"\", c.strip().lower()) for c in df.columns}\n",
    "    inv = {v: k for k, v in cols.items()}\n",
    "\n",
    "    # Common variants: \"Word 1\", \"Word 2\", \"Human (mean)\"\n",
    "    w1_candidates = [\"word1\", \"word_1\", \"word 1\", \"wordone\", \"worda\"]\n",
    "    w2_candidates = [\"word2\", \"word_2\", \"word 2\", \"wordtwo\", \"wordb\"]\n",
    "    s_candidates  = [\"score\", \"similarity\", \"human\", \"human(mean)\", \"mean\", \"gold\", \"rating\"]\n",
    "\n",
    "    def find_col(cands):\n",
    "        for c in cands:\n",
    "            key = re.sub(r\"\\s+\", \"\", c.strip().lower())\n",
    "            if key in inv:\n",
    "                return inv[key]\n",
    "        return None\n",
    "\n",
    "    w1_col = find_col(w1_candidates) or df.columns[0]\n",
    "    w2_col = find_col(w2_candidates) or df.columns[1]\n",
    "\n",
    "    score_col = find_col(s_candidates)\n",
    "    if score_col is None:\n",
    "        # fallback: last numeric column\n",
    "        num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "        if not num_cols:\n",
    "            raise ValueError(\"Couldn't find a numeric similarity score column in the WordSim CSV.\")\n",
    "        score_col = num_cols[-1]\n",
    "\n",
    "    out = df[[w1_col, w2_col, score_col]].copy()\n",
    "    out.columns = [\"word1\", \"word2\", \"score\"]\n",
    "    out[\"word1\"] = out[\"word1\"].astype(str).str.strip()\n",
    "    out[\"word2\"] = out[\"word2\"].astype(str).str.strip()\n",
    "    out[\"score\"] = pd.to_numeric(out[\"score\"], errors=\"coerce\")\n",
    "    out = out.dropna(subset=[\"score\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def cosine_sim_matrix(vectors: np.ndarray) -> np.ndarray:\n",
    "    eps = 1e-12\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    return vectors / np.maximum(norms, eps)\n",
    "\n",
    "def wordsim_metrics_numpy(vectors: np.ndarray, word2idx: Dict[str,int], wordsim_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Returns MSE (after rescaling), Spearman correlation, and counts.\n",
    "    Rescaling:\n",
    "      - model cosine in [-1,1] -> [0,1] via (x+1)/2\n",
    "      - human score normalized to [0,1] via min-max in the dataset\n",
    "    \"\"\"\n",
    "    mat_norm = cosine_sim_matrix(vectors)\n",
    "\n",
    "    model_sims = []\n",
    "    human = []\n",
    "    skipped = 0\n",
    "\n",
    "    for w1, w2, s in wordsim_df[[\"word1\",\"word2\",\"score\"]].itertuples(index=False):\n",
    "        if w1 not in word2idx or w2 not in word2idx:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        i, j = word2idx[w1], word2idx[w2]\n",
    "        sim = float(mat_norm[i] @ mat_norm[j])  # cosine\n",
    "        model_sims.append(sim)\n",
    "        human.append(float(s))\n",
    "\n",
    "    model_sims = np.array(model_sims, dtype=np.float64)\n",
    "    human = np.array(human, dtype=np.float64)\n",
    "\n",
    "    if len(human) == 0:\n",
    "        return {\"used\": 0, \"skipped\": skipped, \"mse\": np.nan, \"spearman\": np.nan, \"p\": np.nan}\n",
    "\n",
    "    # Normalize to comparable scale for MSE\n",
    "    model_01 = (model_sims + 1.0) / 2.0\n",
    "    h_min, h_max = float(human.min()), float(human.max())\n",
    "    human_01 = (human - h_min) / max(h_max - h_min, 1e-12)\n",
    "\n",
    "    mse = float(np.mean((model_01 - human_01) ** 2))\n",
    "    rho, p = spearmanr(model_sims, human)  # Spearman is rank-based, so no rescale needed\n",
    "    return {\"used\": len(human), \"skipped\": skipped, \"mse\": mse, \"spearman\": float(rho), \"p\": float(p)}\n",
    "\n",
    "def wordsim_metrics_kv(kv, wordsim_df: pd.DataFrame):\n",
    "    words = kv.index_to_key\n",
    "    mat = kv.vectors.astype(np.float32)\n",
    "    norms = np.linalg.norm(mat, axis=1, keepdims=True)\n",
    "    mat_norm = mat / np.maximum(norms, 1e-12)\n",
    "    key_to_index = kv.key_to_index\n",
    "\n",
    "    model_sims = []\n",
    "    human = []\n",
    "    skipped = 0\n",
    "\n",
    "    for w1, w2, s in wordsim_df[[\"word1\",\"word2\",\"score\"]].itertuples(index=False):\n",
    "        if w1 not in key_to_index or w2 not in key_to_index:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        i, j = key_to_index[w1], key_to_index[w2]\n",
    "        sim = float(mat_norm[i] @ mat_norm[j])\n",
    "        model_sims.append(sim)\n",
    "        human.append(float(s))\n",
    "\n",
    "    model_sims = np.array(model_sims, dtype=np.float64)\n",
    "    human = np.array(human, dtype=np.float64)\n",
    "\n",
    "    if len(human) == 0:\n",
    "        return {\"used\": 0, \"skipped\": skipped, \"mse\": np.nan, \"spearman\": np.nan, \"p\": np.nan}\n",
    "\n",
    "    model_01 = (model_sims + 1.0) / 2.0\n",
    "    h_min, h_max = float(human.min()), float(human.max())\n",
    "    human_01 = (human - h_min) / max(h_max - h_min, 1e-12)\n",
    "\n",
    "    mse = float(np.mean((model_01 - human_01) ** 2))\n",
    "    rho, p = spearmanr(model_sims, human)\n",
    "    return {\"used\": len(human), \"skipped\": skipped, \"mse\": mse, \"spearman\": float(rho), \"p\": float(p)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47ed655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim rows: 353\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>admission</td>\n",
       "      <td>ticket</td>\n",
       "      <td>5.5360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alcohol</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>4.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aluminum</td>\n",
       "      <td>metal</td>\n",
       "      <td>6.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>announcement</td>\n",
       "      <td>effort</td>\n",
       "      <td>2.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>announcement</td>\n",
       "      <td>news</td>\n",
       "      <td>7.1875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word1      word2   score\n",
       "0     admission     ticket  5.5360\n",
       "1       alcohol  chemistry  4.1250\n",
       "2      aluminum      metal  6.6250\n",
       "3  announcement     effort  2.0625\n",
       "4  announcement       news  7.1875"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# RUN EVERYTHING (4 rows) + WordSim353 MSE/Spearman\n",
    "# =========================\n",
    "# Assumes you already have:\n",
    "# - word2idx, idx2word, freq, encoded (from your corpus)\n",
    "# - sections, SEMANTIC_SECTION, SYNTACTIC_SECTION (from analogy file)\n",
    "# - training functions:\n",
    "#     train_skipgram_softmax(...)\n",
    "#     train_skipgram_neg(...)\n",
    "#     train_glove(...)\n",
    "#\n",
    "# Provide your WordSim353 CSV path here:\n",
    "WORDSIM_PATH = \"wordsim353crowd.csv\"   # <-- change if needed\n",
    "\n",
    "wordsim_df = load_wordsim353_csv(WORDSIM_PATH)\n",
    "print(\"WordSim rows:\", len(wordsim_df))\n",
    "wordsim_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9409e2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipgram Softmax] epoch 1/1 avg_loss=6.3748\n",
      "[Skipgram NEG] epoch 1/2 avg_loss=2.7267\n",
      "[Skipgram NEG] epoch 2/2 avg_loss=2.5282\n",
      "[GloVe] epoch 1/20 avg_loss=0.0550\n",
      "[GloVe] epoch 4/20 avg_loss=0.0246\n",
      "[GloVe] epoch 8/20 avg_loss=0.0198\n",
      "[GloVe] epoch 12/20 avg_loss=0.0167\n",
      "[GloVe] epoch 16/20 avg_loss=0.0145\n",
      "[GloVe] epoch 20/20 avg_loss=0.0129\n",
      "Gensim status: OK\n"
     ]
    }
   ],
   "source": [
    "# ---- Train the 3 from-scratch models (your previous functions) ----\n",
    "WINDOW_SIZE = 5\n",
    "D = 100\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1) Skip-gram Softmax\n",
    "W_sg, loss_sg, t_sg = train_skipgram_softmax(encoded, len(idx2word), D=D, window_size=WINDOW_SIZE, epochs=1, lr=0.05)\n",
    "sem_sg, syn_sg = eval_sem_syn_numpy(W_sg, word2idx, idx2word, sections, SEMANTIC_SECTION, SYNTACTIC_SECTION)\n",
    "ws_sg = wordsim_metrics_numpy(W_sg, word2idx, wordsim_df)\n",
    "results.append((\"Skipgram\", WINDOW_SIZE, loss_sg, t_sg, syn_sg[\"accuracy\"], sem_sg[\"accuracy\"], ws_sg[\"mse\"], ws_sg[\"spearman\"]))\n",
    "\n",
    "# 2) Skip-gram NEG\n",
    "W_sgns, loss_sgns, t_sgns = train_skipgram_neg(encoded, len(idx2word), freq, word2idx, D=D, window_size=WINDOW_SIZE, epochs=2, lr=0.05, K=5)\n",
    "sem_sgns, syn_sgns = eval_sem_syn_numpy(W_sgns, word2idx, idx2word, sections, SEMANTIC_SECTION, SYNTACTIC_SECTION)\n",
    "ws_sgns = wordsim_metrics_numpy(W_sgns, word2idx, wordsim_df)\n",
    "results.append((\"Skipgram (NEG)\", WINDOW_SIZE, loss_sgns, t_sgns, syn_sgns[\"accuracy\"], sem_sgns[\"accuracy\"], ws_sgns[\"mse\"], ws_sgns[\"spearman\"]))\n",
    "\n",
    "# 3) GloVe scratch\n",
    "W_glove, loss_glove, t_glove = train_glove(encoded, len(idx2word), D=D, window_size=WINDOW_SIZE, epochs=20, lr=0.05)\n",
    "sem_glove, syn_glove = eval_sem_syn_numpy(W_glove, word2idx, idx2word, sections, SEMANTIC_SECTION, SYNTACTIC_SECTION)\n",
    "ws_glove = wordsim_metrics_numpy(W_glove, word2idx, wordsim_df)\n",
    "results.append((\"GloVe (scratch)\", WINDOW_SIZE, loss_glove, t_glove, syn_glove[\"accuracy\"], sem_glove[\"accuracy\"], ws_glove[\"mse\"], ws_glove[\"spearman\"]))\n",
    "\n",
    "# 4) GloVe (Gensim) row\n",
    "# If gensim isn't available / no internet to download, we still create a row with NaNs.\n",
    "glove_kv = None\n",
    "gensim_status = \"OK\"\n",
    "try:\n",
    "    import gensim.downloader as api\n",
    "    glove_kv = api.load(\"glove-wiki-gigaword-100\")\n",
    "except Exception as e:\n",
    "    gensim_status = f\"UNAVAILABLE: {type(e).__name__}: {e}\"\n",
    "\n",
    "if glove_kv is not None:\n",
    "    sem_g = evaluate_analogies_kv(glove_kv, sections[SEMANTIC_SECTION])\n",
    "    syn_g = evaluate_analogies_kv(glove_kv, sections[SYNTACTIC_SECTION])\n",
    "    ws_g  = wordsim_metrics_kv(glove_kv, wordsim_df)\n",
    "    results.append((\"GloVe (Gensim)\", \"-\", np.nan, np.nan, syn_g[\"accuracy\"], sem_g[\"accuracy\"], ws_g[\"mse\"], ws_g[\"spearman\"]))\n",
    "else:\n",
    "    results.append((\"GloVe (Gensim)\", \"-\", np.nan, np.nan, np.nan, np.nan, np.nan, np.nan))\n",
    "\n",
    "print(\"Gensim status:\", gensim_status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "865ea308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model\t\t\tWindow\tTrainLoss\tTrainTime(s)\tSynAcc\t\tSemAcc\t\tWordSim MSE\tWordSim Spearman\n",
      "Skipgram        \t5\t6.3748\t\t361.35\t\t0.0000\t0.0000\t0.2625\t\t-0.0008\n",
      "Skipgram (NEG)  \t5\t2.5282\t\t99.64\t\t0.0040\t0.0000\t0.2061\t\t0.0383\n",
      "GloVe (scratch) \t5\t0.0129\t\t57.78\t\t0.0079\t0.0000\t0.1234\t\t0.0630\n",
      "GloVe (Gensim)  \t-\t-\t\t-\t\t0.5545\t0.0000\t0.1552\t\t0.4867\n"
     ]
    }
   ],
   "source": [
    "# ---- Pretty table output ----\n",
    "print(\"\\nModel\\t\\t\\tWindow\\tTrainLoss\\tTrainTime(s)\\tSynAcc\\t\\tSemAcc\\t\\tWordSim MSE\\tWordSim Spearman\")\n",
    "for (name, win, loss, tsec, synacc, semacc, mse, rho) in results:\n",
    "    loss_str = f\"{loss:.4f}\" if isinstance(loss, (int,float)) and not np.isnan(loss) else \"-\"\n",
    "    t_str    = f\"{tsec:.2f}\" if isinstance(tsec, (int,float)) and not np.isnan(tsec) else \"-\"\n",
    "    syn_str  = f\"{synacc:.4f}\" if isinstance(synacc, (int,float)) and not np.isnan(synacc) else \"-\"\n",
    "    sem_str  = f\"{semacc:.4f}\" if isinstance(semacc, (int,float)) and not np.isnan(semacc) else \"-\"\n",
    "    mse_str  = f\"{mse:.4f}\" if isinstance(mse, (int,float)) and not np.isnan(mse) else \"-\"\n",
    "    rho_str  = f\"{rho:.4f}\" if isinstance(rho, (int,float)) and not np.isnan(rho) else \"-\"\n",
    "    print(f\"{name:<16}\\t{win}\\t{loss_str}\\t\\t{t_str}\\t\\t{syn_str}\\t{sem_str}\\t{mse_str}\\t\\t{rho_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce35c42",
   "metadata": {},
   "source": [
    "From the data above, we can see that Skipgram (Word2Vec) takes the longest training time and highest training lost due to it having to compute using the whole vocabulary for every training pair (computational workload is higher). Using negative sampling, we can see that it greatly decreases the training loss and training time. Lastly, GloVe converges the fastest and with minimal loss due to optimized weighted global statistic rather than predicting each word at a time. Gensim is pretrained and does not come with training loss or time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10896ff4",
   "metadata": {},
   "source": [
    "In terms of similarity, GloVe gensim performed the best with the lowest MSE largely duye to a higher scale training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0627b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Suppose you have:\n",
    "#   W_sgns (V,D) and word2idx dict\n",
    "# OR W_glove (V,D) and word2idx dict\n",
    "np.save(\"app/data/embeddings.npy\", W_glove.astype(np.float32))   # choose your model here\n",
    "\n",
    "with open(\"app/data/word2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word2idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38d73370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved contexts: 4403\n",
      "Sample:\n",
      "- Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\n",
      "- They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.\n",
      "- Dursley was the director of a firm called Grunnings, which made drills.\n",
      "- He was a big, beefy man with hardly any neck, although he did have a very large mustache.\n",
      "- Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "RAW_TEXT_PATH = \"archive/SorcerersStone.txt\"      # <-- change this\n",
    "OUTPUT_PATH   = \"app/data/corpus.txt\"\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    # simple sentence splitter (good enough for assignment)\n",
    "    return re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "with open(RAW_TEXT_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    raw = f.read()\n",
    "\n",
    "raw = clean_text(raw)\n",
    "sentences = split_into_sentences(raw)\n",
    "\n",
    "# Filter very short sentences\n",
    "sentences = [s for s in sentences if len(s.split()) >= 5]\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in sentences:\n",
    "        f.write(s + \"\\n\")\n",
    "\n",
    "print(\"Saved contexts:\", len(sentences))\n",
    "print(\"Sample:\")\n",
    "for s in sentences[:5]:\n",
    "    print(\"-\", s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
